#!/bin/bash

PATH="${PATH}:/usr/local/bin"
export PATH
python_pkgs=""
python_exec=""
#
# To make sure.
#
exit_out()
{
	echo $1
	exit $2
}

usage()
{
	echo "$1 Usage:"
	echo "--python_exec_path: Python to set via alternatives"
	echo "--python_pkgs: comma seprated list of python packages to install"
	source test_tools/general_setup --usage
        exit 1
}

install_tools()
{
	show_usage=0
	#
	# Clone the repo that contains the common code and tools
	#
	tools_git=https://github.com/redhat-performance/test_tools-wrappers

	found=0
	for arg in "$@"; do
		if [ $found -eq 1 ]; then
			tools_git=$arg
			found=0
		fi
		if [[ $arg == "--tools_git" ]]; then
			found=1
		fi
		#
		# We do the usage check here, as we do not want to be calling
		# the common parsers then checking for usage here.  Doing so will
		# result in the script exiting with out giving the test options.
		#
		if [[ $arg == "--usage" ]]; then
			show_usage=1
		fi
	done

	#
	# Check to see if the test tools directory exists.  If it does, we do not need to
	# clone the repo.
	#
	if [ ! -d "test_tools" ]; then
		git clone $tools_git test_tools
		if [ $? -ne 0 ]; then
			exit_out "pulling git $tools_git failed." 1
		fi
	fi

	if [ $show_usage -eq 1 ]; then
		usage $1
	fi
}

generate_csv_file()
{
	re='^[0-9]+$'
	instance=0
	float=0
	ivalue=0
	fvalue=0.0
	test_name=""
	unit=""
	reduce=0
	res_count=0
	value_sum=0
	while IFS= read -r line
	do
		if [[ $test_name == "" ]]; then
			test_name=$line
			continue
		fi
		if [ -z "$line" ]; then
			let "reduce=$reduce+1"
			if [[ $reduce -eq 2 ]]; then
				results=`echo "${value_sum}/${res_count}" | bc -l`
				printf "%s:%.2f:%s\n" $test_name $results $unit >> ${1}.csv
				reduce=0
				res_count=0
				value_sum=0
				test_name=""
			fi
			continue
		fi
		if [[ $line == *"--"* ]] || [[ $line == *"calibrate"* ]] || [[ $line == *"warmup"* ]]; then
			continue
		fi
		value=`echo $line | cut -d' ' -f 4`
		unit=`echo $line | cut -d' ' -f 5`
		let "res_count=${res_count}+1"
		value_sum=`echo "${value}+${value_sum}" | bc -l`
	done < "${1}.results"
	results=`echo "${value_sum}/${res_count}" | bc -l`
	printf "%s:%12.2f:%s\n" $test_name $results $unit >> ${1}.csv
}

pip3_install()
{
	if [ $to_no_pkg_install -eq 0 ]; then
		pip3 install $1
		if [ $? -ne 0 ]; then
			exit_out "pip3 install of $1 failed." 1
		fi
	fi
}
#
# Variables set by general setup.
#
# TOOLS_BIN: points to the tool directory
# to_home_root: home directory
# to_configuration: configuration information
# to_times_to_run: number of times to run the test
# to_pbench: Run the test via pbench
# to_pbench_copy: Copy the data to the pbench repository, not move_it.
# to_puser: User running pbench
# to_run_label: Label for the run
# to_user: User on the test system running the test
# to_sys_type: for results info, basically aws, azure or local
# to_sysname: name of the system
# to_tuned_setting: tuned setting
#

install_tools $0

test_name_run="pyperf"
arguments="$@"

curdir=`pwd`

if [[ $0 == "./"* ]]; then
	chars=`echo $0 | awk -v RS='/' 'END{print NR-1}'`
	if [[ $chars == 1 ]]; then
		run_dir=`pwd`
	else
		run_dir=`echo $0 | cut -d'/' -f 1-${chars} | cut -d'.' -f2-`
		run_dir="${curdir}${run_dir}"
	fi
elif [[ $0 != "/"* ]]; then
	dir=`echo $0 | rev | cut -d'/' -f2- | rev`
	run_dir="${curdir}/${dir}"
else
	chars=`echo $0 | awk -v RS='/' 'END{print NR-1}'`
	run_dir=`echo $0 | cut -d'/' -f 1-${chars}`
	if [[ $run_dir != "/"* ]]; then
		run_dir=${curdir}/${run_dir}
	fi
fi

# Gather hardware information
${curdir}/test_tools/gather_data ${curdir}


if [ ! -f "/tmp/pyperf.out" ]; then
        command="${0} $@"
        echo $command
        $command &> /tmp/pyperf.out
	rtc=$?
	if [ -f /tmp/pyperf.out ]; then
		echo =================================
		echo Output from the test.
		echo =================================
        	cat /tmp/pyperf.out
        	rm /tmp/pyperf.out
	fi
        exit $rtc
fi


if [ -d "workloads" ]; then
	#
	# If running from zathras, workloads will be symlinked to
	# to /mnt.  Which is done due to azure having a very small
	# user space.
	#
	start_dir=`pwd`
	cd workloads
	for file in `ls ${start_dir}`; do
		if [[ ! -f $file ]] && [[ ! -d $file ]]; then
			ln -s $start_dir/* .
		fi
	done
fi

source test_tools/general_setup "$@"

ARGUMENT_LIST=(
	"python_exec"
        "python_pkgs"
)

NO_ARGUMENTS=(
        "usage"
)

# read arguments
opts=$(getopt \
	--longoptions "$(printf "%s:," "${ARGUMENT_LIST[@]}")" \
        --longoptions "$(printf "%s," "${NO_ARGUMENTS[@]}")" \
        --name "$(basename "$0")" \
        --options "h" \
        -- "$@"
)

eval set --$opts

while [[ $# -gt 0 ]]; do
	case "$1" in
		--python_exec)
			python_exec=$2
			shift 2
		;;
		--python_pkgs)
			python_pkgs=$2
			shift 2
		;;
		--usage)
			usage $0
		;;
		-h)
			usage $0
		;;
		--)
			break
		;;
		*)
			echo option not found $1
			usage $0
		;;
	esac
done

if [ $to_pbench -eq 0 ]; then
	rm -rf pyperformance
	git clone https://github.com/python/pyperformance
	if [ $? -ne 0 ]; then
		exit_out "Cloning of https://github.com/python/pyperformance failed." 1
	fi
	cd pyperformance
	git checkout tags/1.0.4
	if [ $? -ne 0 ]; then
		exit_out "Checkout of 1.0.4 failed." 1
	fi
	if [[ ${python_pkgs} != "" ]]; then
		pkg_list=`echo $python_pkgs | sed "s/,/ /g"`
		test_tools/package_install --packages "$python_pkgs" --no_packages $to_no_pkg_install
	fi
	if [[ $python_exec != "" ]]; then
		if [[ ! -f $python_exec ]]; then
			exit_out "Error: Designated python executable, $python_exec, not present"
		fi
		#
		# Remove the existing (if any) default python.
		#
		alternatives --remove-all python
 		alternatives --install /usr/bin/python python $python_exec 1
	fi
	#
	# Install pip/pip3
	#
	wget https://bootstrap.pypa.io/get-pip.py
	python3 ./get-pip.py
	pip3_install psutil
	pip3_install packaging
	pip3_install pyparsing
	pip3_install pyperf
	pip3_install toml

	cpus=`cat /proc/cpuinfo | grep processor | wc -l`
	cous=1
	mkdir python_results
	
	pyresults=python_results/pyperf_out_$(date "+%Y.%m.%d-%H.%M.%S")
	pwd > /tmp/dave_debug
	echo python3 -m pyperformance run --output  ${pyresults}.json >> /tmp/dave_debug
	python3 -m pyperformance run --output  ${pyresults}.json
	if [ $? -ne 0 ]; then
		exit_out "Failed: python3 -m pyperformance run --output  ${pyresults}.json" 1
	fi
	echo python3 -m pyperf dump  ${pyresults}.json >> /tmp/dave_debug
	python3 -m pyperf dump  ${pyresults}.json > ${pyresults}.results
	if [ $? -ne 0 ]; then
		echo "Failed: python3 -m pyperf dump  ${pyresults}.json > ${pyresults}.results" 1
	fi
	generate_csv_file ${pyresults}
else
	source ~/.bashrc
	arguments="${arguments} --test_iterations ${to_times_to_run}"
	cd $curdir
	echo $TOOLS_BIN/execute_via_pbench --cmd_executing "$0" $arguments --test ${test_name_run} --spacing 11 --pbench_stats $to_pstats
	$TOOLS_BIN/execute_via_pbench --cmd_executing "$0" $arguments --test ${test_name_run} --spacing 11 --pbench_stats $to_pstats
	if [ $? -ne 0  ]; then
		exit_out "Failed: $TOOLS_BIN/execute_via_pbench --cmd_executing "$0" $arguments --test ${test_name_run} --spacing 11 --pbench_stats $to_pstats"
	fi
fi


#
# Process the data.
#
if [ $to_pbench -eq 1 ]; then
	results_prefix=$to_puser"_instance_"$to_configuration
	dir=`ls -rtd /var/lib/pbench-agent/pyperf* | tail -1`
	cd $dir
	echo $dir > /tmp/pbench_debug
else
	RESULTSDIR=results_${test_name_run}_${to_tuned_setting}_$(date "+%Y.%m.%d-%H.%M.%S")
	mkdir /tmp/${RESULTSDIR}
	cp python_results/* /tmp/${RESULTSDIR}
	cd /tmp
	rm results_${test_name_run}_${to_tuned_setting}
	ln -s ${RESULTSDIR} results_${test_name_run}_${to_tuned_setting}
	mv  ${test_name_run}_*.out ${RESULTSDIR}
	lines=`wc -l ${RESULTSDIR}/*.csv | cut -d' ' -f1`
	if [ $? -ne 0 ]; then
		echo Failed >> ${RESULTSDIR}/test_results_report
	else
		echo Ran >> ${RESULTSDIR}/test_results_report
	fi
	cp /tmp/pyperf.out ${RESULTSDIR}
	cp ${curdir}/meta_data*.yml ${RESULTSDIR}
	${curdir}/test_tools/move_data $curdir ${RESULTSDIR}
	rm -f results_pbench.tar
	working_dir=`ls -rtd /tmp/results*${test_name}* | grep -v tar | tail -1`
	find $working_dir  -type f | tar --transform 's/.*\///g' -cf results_pbench.tar --files-from=/dev/stdin
	tar hcf results_${test_name_run}_${to_tuned_setting}.tar results_${test_name_run}_${to_tuned_setting}
fi
exit 0
