#!/bin/bash

#
# To make sure.
#
alternatives --set python /usr/bin/python3
usage()
{
	echo "$1 Usage:"
	source test_tools/general_setup --usage
        exit 0
}

install_tools()
{
	show_usage=0
	#
	# Clone the repo that contains the common code and tools
	#
	tools_git=https://github.com/redhat-performance/test_tools-wrappers

	found=0
	for arg in "$@"; do
		if [ $found -eq 1 ]; then
			tools_git=$arg
			found=0
		fi
		if [[ $arg == "--tools_git" ]]; then
			found=1
		fi
		#
		# We do the usage check here, as we do not want to be calling
		# the common parsers then checking for usage here.  Doing so will
		# result in the script exiting with out giving the test options.
		#
		if [[ $arg == "--usage" ]]; then
			show_usage=1
		fi
	done

	#
	# Check to see if the test tools directory exists.  If it does, we do not need to
	# clone the repo.
	#
	if [ ! -d "test_tools" ]; then
		git clone $tools_git test_tools
		if [ $? -ne 0 ]; then
			echo pulling git $tools_git failed.
			exit
		fi
	fi

	if [ $show_usage -eq 1 ]; then
		usage $1
	fi
}

generate_csv_file()
{
	re='^[0-9]+$'
	instance=0
	float=0
	ivalue=0
	fvalue=0.0
	test_name=""
	unit=""
	reduce=0
	res_count=0
	value_sum=0
	while IFS= read -r line
	do
		if [[ $test_name == "" ]]; then
			test_name=$line
			continue
		fi
		if [ -z "$line" ]; then
			let "reduce=$reduce+1"
			if [[ $reduce -eq 2 ]]; then
				results=`echo "${value_sum}/${res_count}" | bc -l`
				printf "%s %.2f\n" $test_name $results >> ${1}.csv
				reduce=0
				res_count=0
				value_sum=0
				test_name=""
			fi
			continue
		fi
		if [[ $line == *"--"* ]] || [[ $line == *"calibrate"* ]] || [[ $line == *"warmup"* ]]; then
			continue
		fi
		value=`echo $line | cut -d' ' -f 4`
		let "res_count=${res_count}+1"
		value_sum=`echo "${value}+${value_sum}" | bc -l`
	done < "${1}.results"
	results=`echo "${value_sum}/${res_count}" | bc -l`
	printf "%s %12.2f\n" $test_name $results >> ${1}.csv
}

#
# Variables set by general setup.
#
# TOOLS_BIN: points to the tool directory
# to_home_root: home directory
# to_configuration: configuration information
# to_times_to_run: number of times to run the test
# to_pbench: Run the test via pbench
# to_pbench_copy: Copy the data to the pbench repository, not move_it.
# to_puser: User running pbench
# to_run_label: Label for the run
# to_user: User on the test system running the test
# to_sys_type: for results info, basically aws, azure or local
# to_sysname: name of the system
# to_tuned_setting: tuned setting
#

install_tools $0

test_name_run="pyperf"
arguments="$@"

curdir=`pwd`
if [[ $0 == "./"* ]]; then
	chars=`echo $0 | awk -v RS='/' 'END{print NR-1}'`
	if [[ $chars == 1 ]]; then
		run_dir=`pwd`
	else
		run_dir=`echo $0 | cut -d'/' -f 1-${chars} | cut -d'.' -f2-`
		run_dir="${curdir}${run_dir}"
	fi
else
	chars=`echo $0 | awk -v RS='/' 'END{print NR-1}'`
	run_dir=`echo $0 | cut -d'/' -f 1-${chars}`
fi

if [ ! -f "/tmp/pyperf.out" ]; then
        command="${0} $@"
        echo $command
        $command &> /tmp/pyperf.out
        cat /tmp/pyperf.out
        rm /tmp/pyperf.out
        exit
fi


if [ -d "workloads" ]; then
	#
	# If running from zathras, workloads will be symlinked to
	# to /mnt.  Which is done due to azure having a very small
	# user space.
	#
	start_dir=`pwd`
	cd workloads
	ln -s $start_dir/* .
fi

source test_tools/general_setup "$@"

NO_ARGUMENTS=(
        "powers_2"
        "usage"
)

# read arguments
opts=$(getopt \
        --longoptions "$(printf "%s," "${NO_ARGUMENTS[@]}")" \
        --name "$(basename "$0")" \
        --options "h" \
        -- "$@"
)

eval set --$opts

while [[ $# -gt 0 ]]; do
	case "$1" in
		--usage)
			usage $0
		;;
		-h)
			usage $0
		;;
		--)
			break
		;;
		*)
			echo option not found $1
			usage $0
		;;
	esac
done


if [ $to_pbench -eq 0 ]; then
	rm -rf pyperformance
	git clone https://github.com/python/pyperformance
	cd pyperformance
	git checkout tags/1.0.4
	dnf install -y python36 python36-devel
	dnf install -y python38 python38-devel
	pip3 install psutil
	pip3 install packaging
	pip3 install pyparsing
	pip3 install pyperf
	pip3 install toml

	cpus=`cat /proc/cpuinfo | grep processor | wc -l`
	cous=1
	mkdir python_results
	
	pyresults=python_results/pyperf_out_$(date "+%Y.%m.%d-%H.%M.%S")
	pwd > /tmp/dave_debug
	echo python3 -m pyperformance run --output  ${pyresults}.json >> /tmp/dave_debug
	python3 -m pyperformance run --output  ${pyresults}.json
	echo python3 -m pyperf dump  ${pyresults}.json >> /tmp/dave_debug
	python3 -m pyperf dump  ${pyresults}.json > ${pyresults}.results

	generate_csv_file ${pyresults}
else
	source ~/.bashrc
	arguments="${arguments} --test_iterations ${to_times_to_run}"
	cd $curdir
	echo $TOOLS_BIN/execute_via_pbench --cmd_executing "$0" $arguments --test ${test_name_run} --spacing 11 --pbench_stats $to_pstats
	$TOOLS_BIN/execute_via_pbench --cmd_executing "$0" $arguments --test ${test_name_run} --spacing 11 --pbench_stats $to_pstats
fi


#
# Process the data.
#
if [ $to_pbench -eq 1 ]; then
	results_prefix=$to_puser"_instance_"$to_configuration
	dir=`ls -rtd /var/lib/pbench-agent/pyperf* | tail -1`
	cd $dir
	echo $dir > /tmp/pbench_debug
else
	RESULTSDIR=results_${test_name_run}_${to_tuned_setting}_$(date "+%Y.%m.%d-%H.%M.%S")
	mkdir /tmp/${RESULTSDIR}
	cp python_results/* /tmp/${RESULTSDIR}
	cd /tmp
	rm results_${test_name_run}_${to_tuned_setting}
	ln -s ${RESULTSDIR} results_${test_name_run}_${to_tuned_setting}
	mv  ${test_name_run}_*.out ${RESULTSDIR}
	lines=`wc -l ${RESULTSDIR}/*.csv | cut -d' ' -f1`
	if [ $? -ne 0 ]; then
		echo Failed >> ${RESULTSDIR}/test_results_report
	else
		echo Ran >> ${RESULTSDIR}/test_results_report
	fi
	cp /tmp/pyperf.out ${RESULTSDIR}
	cp ${curdir}/meta_data.yml ${RESULTSDIR}
	tar hcf results_${test_name_run}_${to_tuned_setting}.tar results_${test_name_run}_${to_tuned_setting}
	cp  results_${test_name_run}_${to_tuned_setting}.tar results_pbench_${test_name_run}_${to_tuned_setting}.tar
fi
exit 0
